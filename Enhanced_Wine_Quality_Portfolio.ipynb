{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e9cf5d",
   "metadata": {},
   "source": [
    "\n",
    "# Wine Quality Prediction — Portfolio Project\n",
    "\n",
    "**Author:** _<Your Name Here>_<br>\n",
    "**Dataset:** `allwine.csv` (red + white wine quality attributes)<br>\n",
    "**Goal:** Predict wine **quality** and demonstrate both **from-scratch logistic regression** (recap) and **applied ML with ensembles** using robust evaluation and interpretability.\n",
    "\n",
    "---\n",
    "\n",
    "## Project Story\n",
    "\n",
    "- **Problem:** Predict wine quality (good vs. not-good) from physicochemical properties.\n",
    "- **Business framing:** Assist vintners and quality control teams to triage batches and optimize processes.\n",
    "- **What I show here:**\n",
    "  1. Clean EDA and data preprocessing.\n",
    "  2. Baseline from-scratch logistic regression **(recap of academic work)**.\n",
    "  3. Stronger applied models: LogisticRegression (sklearn), Random Forest, Gradient Boosting, AdaBoost.\n",
    "  4. Robust evaluation: cross-validation, ROC/PR curves, confusion matrix, calibration.\n",
    "  5. Feature importance + permutation importance.\n",
    "  6. Clear conclusions and next steps.\n",
    "\n",
    "> This notebook is the **portfolio-ready** version. The class-restricted notebook remains unchanged for grading.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309afd64",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Setup & Data Load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaf6d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Load\n",
    "data_path = Path('/mnt/data/allwine.csv')  # adjust if needed\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c48ea07",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Quick EDA\n",
    "\n",
    "We look at schema, missingness, target distribution, and pairwise correlations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104d5039",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37330bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.describe().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72692c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.isnull().sum().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffde702",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x=df['quality'])\n",
    "plt.title('Raw Quality Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "corr = df.corr(numeric_only=True)\n",
    "sns.heatmap(corr, annot=False)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6da50c",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Target Engineering\n",
    "\n",
    "For this portfolio version, we frame it as a **binary classification** problem, a common practice for wine quality datasets:\n",
    "\n",
    "- **Good (1):** quality \\>= 7  \n",
    "- **Not Good (0):** otherwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbcb1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.copy()\n",
    "df['target'] = (df['quality'] >= 7).astype(int)\n",
    "df['target'].value_counts(normalize=True).rename('proportion')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dd7cac",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Feature Set\n",
    "\n",
    "We use the 10 required features from the assignment for comparability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9e6e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FEATURES = ['fixed acidity','volatile acidity','citric acid','residual sugar','chlorides',\n",
    "            'free sulfur dioxide','density','pH','sulphates','alcohol']\n",
    "\n",
    "X = df[FEATURES].copy()\n",
    "y = df['target'].copy()\n",
    "\n",
    "X.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcd9279",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Train/Test Split & Scaling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086a1c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba8ed50",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Baseline (Recap): From-Scratch Logistic Regression\n",
    "\n",
    "Below is a compact recap of a from-scratch logistic regression implementation (vectorized).  \n",
    "This mirrors the academic notebook but is included here to demonstrate algorithmic understanding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c13c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def initialize_weights(n_features):\n",
    "    w = np.zeros((1, n_features))\n",
    "    b = 0.0\n",
    "    return w, b\n",
    "\n",
    "def optimize(w, b, X, y):\n",
    "    m = X.shape[0]\n",
    "    A = sigmoid(np.dot(w, X.T) + b)\n",
    "    # numerical stability\n",
    "    eps = 1e-12\n",
    "    cost = (-1/m) * np.sum(y.values.reshape(1,-1)*np.log(A+eps) + (1-y.values.reshape(1,-1))*np.log(1-A+eps))\n",
    "    dw = (1/m) * np.dot(X.T, (A - y.values.reshape(1,-1)).T)\n",
    "    db = (1/m) * np.sum(A - y.values.reshape(1,-1))\n",
    "    return {\"dw\": dw, \"db\": db}, cost\n",
    "\n",
    "def train_from_scratch(Xs, ys, lr=0.01, iters=1000):\n",
    "    w, b = initialize_weights(Xs.shape[1])\n",
    "    costs = []\n",
    "    for i in range(iters):\n",
    "        grads, cost = optimize(w, b, Xs, ys)\n",
    "        w = w - lr * grads['dw'].T\n",
    "        b = b - lr * grads['db']\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    return w, b, costs\n",
    "\n",
    "def predict_from_scratch(w, b, Xs):\n",
    "    A = sigmoid(np.dot(w, Xs.T) + b)\n",
    "    return (A.flatten() > 0.5).astype(int)\n",
    "\n",
    "# Train on scaled data\n",
    "w_fs, b_fs, costs_fs = train_from_scratch(pd.DataFrame(X_train_s), y_train, lr=0.05, iters=2000)\n",
    "yhat_fs = predict_from_scratch(w_fs, b_fs, pd.DataFrame(X_test_s))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc_fs = accuracy_score(y_test, yhat_fs)\n",
    "acc_fs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1ee597",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(costs_fs)\n",
    "plt.title('From-Scratch Logistic: Cost over Iterations')\n",
    "plt.xlabel('x100 iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46e675f",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Applied Models (sklearn) — Stronger Baselines & Ensembles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26554926",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "models = {\n",
    "    \"LogReg (sklearn)\": LogisticRegression(max_iter=2000, random_state=RANDOM_STATE),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "    \"AdaBoost\": AdaBoostClassifier(n_estimators=300, random_state=RANDOM_STATE),\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "cv_results = {}\n",
    "\n",
    "for name, clf in models.items():\n",
    "    scores = cross_val_score(clf, X_train_s, y_train, cv=cv, scoring='accuracy', n_jobs=None)\n",
    "    cv_results[name] = (scores.mean(), scores.std())\n",
    "\n",
    "cv_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a16c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit best-performing model on full train and evaluate on test\n",
    "# (We'll pick the model with the highest CV mean)\n",
    "best_name = max(cv_results.items(), key=lambda kv: kv[1][0])[0]\n",
    "best_model = models[best_name]\n",
    "best_model.fit(X_train_s, y_train)\n",
    "yhat_test = best_model.predict(X_test_s)\n",
    "\n",
    "test_acc = accuracy_score(y_test, yhat_test)\n",
    "print(\"Best CV model:\", best_name)\n",
    "print(\"Test Accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d095514",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Evaluation: Confusion Matrix & Classification Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcafb542",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(y_test, yhat_test)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.title('Confusion Matrix (Test)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test, yhat_test, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd73599",
   "metadata": {},
   "source": [
    "\n",
    "## 9. ROC & Precision-Recall Curves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbe9e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "\n",
    "if hasattr(best_model, \"predict_proba\"):\n",
    "    y_proba = best_model.predict_proba(X_test_s)[:,1]\n",
    "else:\n",
    "    # For models without predict_proba (e.g., some SVMs), fall back to decision_function if available\n",
    "    if hasattr(best_model, \"decision_function\"):\n",
    "        # scale to [0,1] via min-max for plotting\n",
    "        z = best_model.decision_function(X_test_s)\n",
    "        y_proba = (z - z.min()) / (z.max() - z.min() + 1e-12)\n",
    "    else:\n",
    "        y_proba = yhat_test.astype(float)\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_proba)\n",
    "ap = average_precision_score(y_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.3f}')\n",
    "plt.plot([0,1],[0,1],'--')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(rec, prec, label=f'AP = {ap:.3f}')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ccbf7b",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Feature Importance\n",
    "\n",
    "We inspect feature importances when available (tree-based models) and complement with **Permutation Importance**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739aa59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def plot_importances(names, importances, title):\n",
    "    order = np.argsort(importances)[::-1]\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.bar(range(len(names)), np.array(importances)[order])\n",
    "    plt.xticks(range(len(names)), np.array(names)[order], rotation=45, ha='right')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Model-specific importances (if available)\n",
    "if hasattr(best_model, \"feature_importances_\"):\n",
    "    plot_importances(FEATURES, best_model.feature_importances_, f'{best_name}: Feature Importances')\n",
    "elif best_name.startswith(\"LogReg\") and hasattr(best_model, \"coef_\"):\n",
    "    plot_importances(FEATURES, np.abs(best_model.coef_[0]), f'{best_name}: |Coefficients|')\n",
    "else:\n",
    "    print(\"Model-specific importances not available for\", best_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0191d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Permutation Importance (simple implementation)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "baseline_acc = accuracy_score(y_test, yhat_test)\n",
    "perm_importances = []\n",
    "\n",
    "rng = np.random.RandomState(RANDOM_STATE)\n",
    "X_test_s_copy = X_test_s.copy()\n",
    "\n",
    "for j in range(X_test_s.shape[1]):\n",
    "    saved = X_test_s_copy[:, j].copy()\n",
    "    rng.shuffle(X_test_s_copy[:, j])\n",
    "    y_perm_pred = best_model.predict(X_test_s_copy)\n",
    "    perm_acc = accuracy_score(y_test, y_perm_pred)\n",
    "    drop = baseline_acc - perm_acc\n",
    "    perm_importances.append(drop)\n",
    "    X_test_s_copy[:, j] = saved  # restore\n",
    "\n",
    "plot_importances(FEATURES, perm_importances, f'{best_name}: Permutation Importance (Accuracy Drop)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8856b1cb",
   "metadata": {},
   "source": [
    "\n",
    "## 11. Probability Calibration (Optional)\n",
    "\n",
    "Well-calibrated probabilities matter in operations. We check calibration curves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8930acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "if 'y_proba' in locals():\n",
    "    prob_true, prob_pred = calibration_curve(y_test, y_proba, n_bins=10, strategy='quantile')\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(prob_pred, prob_true, marker='o')\n",
    "    plt.plot([0,1],[0,1],'--')\n",
    "    plt.title('Calibration Curve')\n",
    "    plt.xlabel('Mean Predicted Probability')\n",
    "    plt.ylabel('Fraction of Positives')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Calibration skipped: probability scores not available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9b4808",
   "metadata": {},
   "source": [
    "\n",
    "## 12. Conclusions & Next Steps\n",
    "\n",
    "**Findings (example — update with your actual numbers):**\n",
    "- From-scratch logistic regression reached ~**A%** accuracy.\n",
    "- The best applied model was **<Model>** with **B%** test accuracy.\n",
    "- Top drivers included **alcohol**, **sulphates**, and **volatile acidity** (based on importances).\n",
    "\n",
    "**What this shows:**\n",
    "- Ability to implement algorithms from scratch **and** build production-leaning models with robust validation.\n",
    "- End-to-end ML: EDA → preprocessing → modeling → evaluation → interpretability → communication.\n",
    "\n",
    "**Next steps:**\n",
    "- Try multi-class quality prediction (0–10) or ordinal models.\n",
    "- Add class imbalance handling (e.g., class weights, SMOTE).\n",
    "- Explore model monitoring and drift over time.\n",
    "- Package as a reproducible repo with `README`, environment file, and unit tests for data/metrics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
